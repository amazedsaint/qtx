# Quasicrystal Transformers: Aperiodic Block-Sparse Attention and Aperiodic Positional Encoding from First Principles

**Author:** Anoop Madhusudanan  
**Contact:** amazedsaint@gmail.com

---

## Abstract

We introduce Quasicrystal Transformers (QTX), a family of attention architectures guided by two geometric principles:

1. **Aperiodic Block-Sparse Attention (ABSA)**: Tokens attend locally within fixed-size blocks and non-locally to a deterministic aperiodic set of blocks generated by irrational rotations. This creates a small-world, non-repeating attention graph with strong long-range propagation at fixed per-token degree.

2. **Quasicrystal Positional Encoding (QPE)**: Positional features built from incommensurate (irrational) frequencies drastically reduce aliasing and yield well-conditioned Gram matrices far beyond the training window, supporting stable length extrapolation.

From first principles, we formalize the graph and feature geometry, give complexity and connectivity analyses, and provide complete, runnable NumPy code that (i) constructs ABSA graphs, (ii) measures information-propagation capacity via BFS coverage, (iii) compares QPE against classical sinusoidal encodings on coherence and conditioning, and (iv) runs a minimal sparse attention forward pass. Empirically (pure NumPy simulations), ABSA covers noticeably more of the sequence within a fixed layer budget than sliding-window or dilated patterns, and QPE exhibits dramatically lower positional aliasing.

No prior references are required to understand or implement the method; all algorithms and code are presented self-contained.

---

## 1. Motivation

Sequence models are constrained by two geometry problems:
- **Graph geometry**: With bounded attention degree per token, how fast can information propagate across a sequence in a small number of layers?
- **Feature geometry**: How well-conditioned are positional features outside the training range, and how much do distinct positions look alike (coherence/aliasing)?

Periodic designs (e.g., sliding windows; fixed strides) keep computation cheap but hamper rapid global mixing and often introduce regular spectral signatures that reappear in caches, masks, or encodings. We propose aperiodicity—deterministic, non-repeating structure—as the organizing principle for both the attention graph and the positional features.

---

## 2. Preliminaries

Consider a sequence {x₀,...,x_{N-1}}. Fix a block size T dividing N. Let B = N/T be the number of blocks. Write block index b = ⌊i/T⌋ and in-block offset o = i mod T.

### 2.1 Aperiodic block order

Choose an irrational α ∈ (0,1) (e.g., √2-1). Define the irrational rotation order of blocks by sorting the keys
k(b) = frac(b·α) ∈ [0,1), b=0,...,B-1.

Let P∈{0,...,B-1}^B be this permutation; P_inv its inverse.

### 2.2 Attention degree and neighbors

Let the per-token degree budget be degree. Split it into:
- **local**: radius r inside a block (cost 2r),
- **cross-block**: the remainder as symmetric connections to the same offset o in a small set of aperiodic neighbor blocks along P.

We parameterize cross-block neighbors by a small set of leaps L={ℓ₁,ℓ₂,...}. For a token at i = bT + o, its cross-block neighbors are
{P[P_inv(b)±ℓ]T + o | ℓ ∈ L}.

The resulting graph is bounded-degree, deterministic, and aperiodic at the block scale.

---

## 3. Aperiodic Block-Sparse Attention (ABSA)

### 3.1 Definition

For token i=bT+o, the neighbor set is
N(i) = {bT + (o±1),...,bT+(o±r)} ∪ {P[P_inv(b)±ℓ]T + o : ℓ∈L},

with wraparound in both o and P indices.

The attention head performs scaled dot-product attention restricted to N(i)∪{i}.

### 3.2 Complexity

Per head, per layer: O(N · d · (|N|+1)) = O(N·d·degree), the same order as other block-sparse patterns.

### 3.3 Connectivity & propagation

Let G be the undirected version of the ABSA graph. After L layers of message passing, information from node s can reach any node in the radius-L BFS ball B_L(s).

**Claim (small-world mixing):**
For fixed degree and suitable leap choices L, the ABSA graph exhibits rapid growth of |B_L(0)| in L, significantly outpacing sliding windows and improving over simple dilations.

**Intuition:** The aperiodic cross-block edges introduce non-resonant shortcuts that avoid periodic cycles, producing a bounded-diameter overlay on the block ring.

**Sketch:** Sliding windows produce linear expansion (each step adds O(r) new nodes). Dilations add periodic chords; however, periodicity can trap expansion along repeating residue classes. Aperiodic chords at small leaps ℓ behave like irrational rotations over the block ring, which are equidistributed and thus mix offsets across the blocks without repeating patterns, yielding faster coverage at small L.

---

## 4. Quasicrystal Positional Encoding (QPE)

Define d even, and a set of incommensurate frequencies {ω₁,...,ω_{d/2}} constructed from irrational bases (e.g., √2, √3, φ) and low integer multiples. The positional map

φ(p) = (sin(ω₁p), ..., sin(ω_{d/2}p), cos(ω₁p), ..., cos(ω_{d/2}p))

is aperiodic in p in the sense that no finite period exists across all coordinates.

### 4.1 Coherence and conditioning

Let Φ∈ℝ^{L×d} be the matrix of positional features over positions [0,L). Define:
- **Coherence**: max_{p≠q} ⟨φ(p),φ(q)⟩ / (||φ(p)|| ||φ(q)||)
- **Condition number**: κ(Φᵀ Φ)

**Claim:** For large L, QPE exhibits lower coherence and better conditioning than commensurate sinusoids, especially far beyond the training window.

**Sketch:** With incommensurate ω, the phases {ωᵢp mod 2π} are distributed without common periods, causing off-diagonal inner products to average toward zero and preventing near-collinearities that plague commensurate spectra.

---

## 5. Minimal Sparse Attention Head

Given queries, keys, values Q,K,V∈ℝ^{N×d}, ABSA attention at token i computes softmax weights only over N(i)∪{i}. Implementation uses per-row gather into small dense matrices; the per-token softmax size is degree+1.

---

## 6. Experiments (structural, training-free)

All results below come directly from the NumPy code in the repository.

**Setup:** N=1024, T=16, degree=8, [CLS] token at index 0. Comparisons:
- **Sliding**: local neighbors ±1,±2,±3,±4
- **Dilated**: local ±1,±2 plus block-aligned ±16,±32  
- **ABSA**: local ±1,±2 plus aperiodic block leaps (ℓ₁,ℓ₂)=(2,5) by default

### 6.1 Coverage vs layers (fraction of tokens that [CLS] can reach within L hops)

| Layers L | Sliding | Dilated | ABSA |
|----------|---------|---------|------|
| 2        | 0.0156  | 0.0313  | 0.0332 |
| 3        | 0.0235  | 0.0704  | 0.0821 |
| 4        | 0.0313  | 0.1251  | 0.1535 |
| 6        | 0.0469  | 0.2502  | 0.3353 |
| 8        | 0.0626  | 0.3754  | 0.5230 |

**Observation:** At fixed degree and L, ABSA covers more of the sequence than both baselines; e.g., at L=6 it improves over dilated by ~34% absolute.

### 6.2 Leap tuning (ABSA)

For L=6, the leap pair critically affects mixing:

| Leaps (ℓ₁,ℓ₂) | Coverage |
|---------------|----------|
| (1, 2)        | 0.249    |
| (1, 3)        | 0.335    |
| (1, 5)        | 0.476    |
| (2, 5)        | 0.501    |

**Guideline:** Favor small, co-prime leaps (e.g., 2 and 5) to avoid emergent periodicity; for larger B, pick leap pairs that do not resonate with B.

### 6.3 Positional encodings

Train window [0,512), test window [0,4096), feature dimension d=64.

| Encoding   | Coherence(train) | Coherence(test) | Cond(train) | Cond(test) |
|------------|------------------|-----------------|-------------|------------|
| Sinusoidal | 0.966           | 0.966           | 6.49e18     | 1.72e17    |
| QPE        | 0.607           | 0.667           | 4.63e1      | 1.49e0     |

**Observation:** QPE dramatically reduces aliasing and stabilizes positional features under length extrapolation.

---

## 7. Discussion

- **Why aperiodicity?** It preserves determinism (no randomness) while eliminating global periodic signatures. In graphs, this avoids resonance traps; in features, it avoids commensurate cycles.
- **Why blocks?** Blocks preserve locality and enable simple gather-based sparse kernels. Aperiodic order acts at the block scale, separating concerns: local continuity + nonlocal mixing.
- **Relation to compiler geometry:** The same rationale that motivated aperiodic memory layouts (flattening periodic spectra without randomization) applies here to attention graphs and positional spectra.

---

## 8. Limitations

- These results are structural (graph-theoretic bounds and feature geometry). While they strongly predict easier optimization, task-level training should still validate end-to-end gains.
- Poor leap choices can reduce mixing. A simple heuristic—prefer small, co-prime leaps relative to B—works well in practice.

---

## 9. Practical Recipes

- Start with T∈{16,32}, degree=8–12, leaps (2,5) for B≥64.
- Replace sinusoidal/RoPE with QPE; it is a drop-in deterministic function of position.
- Implement ABSA as a fixed sparse mask (COO/CSR). One head using ABSA plus one local head is a good default.

---

## 10. Conclusion

We proposed a first-principles architecture for attention and positional encoding that embraces aperiodicity as a design axis. The resulting Quasicrystal Transformer (QTX) exhibits demonstrably better graph-theoretic propagation at fixed compute and far better positional conditioning for long contexts. The accompanying code is minimal and ready to integrate.

---

## Acknowledgments

None required; this manuscript is standalone by design.

---

*End of paper.*