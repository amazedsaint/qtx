# Quasicrystal Transformers (QTX)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

**Aperiodic Block-Sparse Attention and Aperiodic Positional Encoding from First Principles**

By Anoop Madhusudanan (amazedsaint@gmail.com)

## Abstract

We introduce Quasicrystal Transformers (QTX), a family of attention architectures guided by two geometric principles:

1. **Aperiodic Block-Sparse Attention (ABSA)**: Tokens attend locally within fixed-size blocks and non-locally to a deterministic aperiodic set of blocks generated by irrational rotations. This creates a small-world, non-repeating attention graph with strong long-range propagation at fixed per-token degree.

2. **Quasicrystal Positional Encoding (QPE)**: Positional features built from incommensurate (irrational) frequencies drastically reduce aliasing and yield well-conditioned Gram matrices far beyond the training window, supporting stable length extrapolation.

## Key Results

- **Better Information Propagation**: ABSA covers significantly more of the sequence within a fixed layer budget compared to sliding-window or dilated patterns
- **Reduced Positional Aliasing**: QPE exhibits dramatically lower positional aliasing and better conditioning than standard sinusoidal encodings
- **Stable Length Extrapolation**: QPE maintains well-conditioned positional features far beyond the training window

## Quick Start

```bash
# Clone the repository
git clone https://github.com/amazedsaint/qtx.git
cd qtx

# Install dependencies
pip install -r requirements.txt

# Run the NumPy implementation demo
python src/qtx_numpy.py
```

## Implementation

### Core Components

- `src/qtx_numpy.py` - Complete NumPy implementation with coverage analysis and positional encoding comparisons
- `src/qtx_torch.py` - PyTorch modules for easy integration
- `examples/` - Jupyter notebooks demonstrating usage and results
- `docs/paper.md` - Full research paper with theoretical foundations

### Key Features

- **Pure NumPy Implementation**: Self-contained, minimal dependencies
- **PyTorch Integration**: Drop-in modules for existing transformer architectures  
- **Comprehensive Analysis**: Graph-theoretic coverage analysis and positional encoding evaluation
- **Reproducible Results**: All experiments can be reproduced with provided code

## Architecture Overview

### Aperiodic Block-Sparse Attention (ABSA)

```python
# Example usage
from src.qtx_torch import ABSAHead

# Create ABSA attention head
attn_head = ABSAHead(d_model=512, degree=8, block=16, leaps=(2,5))

# Apply to sequence
output = attn_head(input_sequence)  # [N, d_model] -> [N, d_model]
```

### Quasicrystal Positional Encoding (QPE)

```python
from src.qtx_torch import quasicrystal_pe

# Generate aperiodic positional encodings
positions = torch.arange(sequence_length)
pos_encodings = quasicrystal_pe(positions, d_model=512)
```

## Results Summary

### Coverage Comparison (L=6 layers)
| Pattern | Coverage |
|---------|----------|
| Sliding | 4.69%    |
| Dilated | 25.02%   |
| **ABSA** | **33.53%** |

### Positional Encoding Comparison
| Encoding | Coherence (Test) | Condition Number (Test) |
|----------|------------------|-------------------------|
| Sinusoidal | 0.966 | 1.72e17 |
| **QPE** | **0.667** | **1.49e0** |

## Paper

The full research paper is available in [`docs/paper.md`](docs/paper.md), including:
- Theoretical foundations and first-principles derivation
- Complete algorithmic descriptions
- Empirical analysis and results
- Implementation details and recipes

## Citation

```bibtex
@article{qtx2024,
  title={Quasicrystal Transformers: Aperiodic Block-Sparse Attention and Aperiodic Positional Encoding from First Principles},
  author={Anoop Madhusudanan},
  year={2024},
  url={https://github.com/amazedsaint/qtx}
}
```

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please see our contributing guidelines and open an issue or pull request.

## Contact

- Author: Anoop Madhusudanan  
- Email: amazedsaint@gmail.com
- GitHub: [@amazedsaint](https://github.com/amazedsaint)